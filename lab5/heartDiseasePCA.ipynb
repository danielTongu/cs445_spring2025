{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0f12fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c806b381",
   "metadata": {},
   "source": [
    "# 📊 Lab 5: Visualizing Principal Component Analysis (PCA) – Heart Disease Dataset\n",
    "\n",
    "## 🧾 Dataset Introduction\n",
    "\n",
    "For this lab, we utilize the **Heart Disease dataset** from Kaggle.\n",
    "\n",
    "It contains 1025 samples with 13 clinical features (e.g., age, cholesterol, thalach) describing patients, along with a target label indicating the presence or absence of heart disease.\n",
    "\n",
    "PCA is particularly useful for this dataset because it enables the reduction of 13 dimensions down to 2 or 3 for visualization, while retaining most of the original variability in the data.  \n",
    "The dataset is balanced and contains numerical features only.\n",
    "\n",
    "### Why PCA for the Heart Disease Dataset?\n",
    "\n",
    "- **High Dimensionality:** With 13 features, visualizing and understanding relationships is challenging. PCA projects the data into fewer dimensions that capture the most variance.\n",
    "- **Class Structure:** The dataset contains two classes (presence or absence of heart disease). PCA helps reveal whether these classes are separable in lower-dimensional space.\n",
    "- **Feature Redundancy:** Some clinical features are correlated. PCA identifies and combines these into principal components, reducing redundancy.\n",
    "- **Interpretability:** By examining the principal components and their loadings (see the Eigenvectors table), we can understand which original features contribute most to the main axes of variation in the data.\n",
    "\n",
    "### What You Will Learn\n",
    "\n",
    "- How to standardize data for PCA.\n",
    "- How to compute and interpret the covariance matrix.\n",
    "- How to extract and visualize eigenvalues and eigenvectors.\n",
    "- How to decide how many principal components to keep.\n",
    "- How to visualize high-dimensional data in 2D using PCA, and interpret the results in the context of the heart disease dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbe825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for PCA workflow\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set global font to Times New Roman for all plots\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 14\n",
    "\n",
    "csv_filename = \"heart.csv\" # downloaded from 'https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset'\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Assign the target labels from the dataset\n",
    "target = df[\"target\"]\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5b237",
   "metadata": {},
   "source": [
    "Before applying PCA, the data is standardized using **StandardScaler**. \n",
    "\n",
    "This step ensures that all features have zero mean and unit variance, preventing any single feature from dominating due to its scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features to zero mean and unit variance (excluding the target column)\n",
    "features = df.drop(columns=[\"target\"])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features.columns)\n",
    "\n",
    "# Display the scaled data\n",
    "print(\"Standardized Features:\")\n",
    "print(X_scaled_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5c0ab",
   "metadata": {},
   "source": [
    "## Compute Covarience Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c38cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = np.cov(X_scaled.T)  # Transpose to get features as rows\n",
    "print(\"Covariance Matrix:\")\n",
    "print(pd.DataFrame(cov_matrix, index=features.columns, columns=features.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe608f",
   "metadata": {},
   "source": [
    "## 🔥 Covariance Matrix Heatmap\n",
    "\n",
    "**What does this image show?**  \n",
    "- The heatmap visualizes the pairwise covariances between all 13 standardized heart disease features.\n",
    "- Diagonal cells (values ≈ 1) represent the variance of each feature after standardization.\n",
    "- Off-diagonal cells show how two features vary together: red for strong positive covariance, blue for strong negative covariance, and white for near-zero (uncorrelated).\n",
    "\n",
    "**What is its highlight?**  \n",
    "- *age* and *trestbps* have a moderate positive covariance (0.27), indicating they tend to increase together.\n",
    "- *thalach* and *exang* show a strong negative covariance (-0.38), meaning as max heart rate increases, exercise-induced angina tends to decrease.\n",
    "- Features like *chol* and *slope* have near-zero covariance (0.06), suggesting little linear relationship.\n",
    "- The heatmap reveals clusters of correlated features, highlighting redundancy and the potential for dimensionality reduction.\n",
    "\n",
    "**What can people learn?**  \n",
    "- Highly correlated features are redundant and can be combined or reduced using PCA.\n",
    "- Features with strong negative covariance may represent opposing trends in the data.\n",
    "- Uncorrelated features provide unique information.\n",
    "- This visualization justifies the use of PCA: it helps identify which features can be compressed into principal components without significant information loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0d2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the covariance matrix as a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    cov_matrix,\n",
    "    xticklabels=features.columns,\n",
    "    yticklabels=features.columns,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    annot=True,\n",
    "    fmt=\".2f\"\n",
    ")\n",
    "plt.title(\"Covariance Matrix Heatmap\")\n",
    "plt.savefig(\"/images/Lab5_DanielTongu_CovarianceHeatmap.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b0434",
   "metadata": {},
   "source": [
    "## Eigenvalues of Principal Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors of the covariance matrix\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues and eigenvectors in descending order\n",
    "sorted_idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_idx]\n",
    "eigenvectors = eigenvectors[:, sorted_idx]\n",
    "\n",
    "print(\"Eigenvalues:\")\n",
    "print(eigenvalues)\n",
    "\n",
    "print(\"\\nEigenvectors:\")\n",
    "Eigenvectors = pd.DataFrame(\n",
    "    eigenvectors,\n",
    "    index=features.columns,\n",
    "    columns=[f'PC{i+1}' for i in range(eigenvectors.shape[1])]\n",
    ")\n",
    "print(Eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f02cf4",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "\n",
    "### 📉 Plot Eigenvalues of Principal Components\n",
    "\n",
    "**What does this image show?**  \n",
    "- The bar chart displays the eigenvalues for each principal component (PC) derived from the covariance matrix of the standardized heart disease dataset.\n",
    "- Each bar's height corresponds to the amount of variance captured by that PC. For this dataset, PC1 has the highest eigenvalue (~2.78), followed by PC2 (~1.56), and so on.\n",
    "\n",
    "**What is its highlight?**  \n",
    "- The first principal component (PC1) alone explains a substantial portion of the total variance, with PC2 also contributing significantly.\n",
    "- Specifically, PC1 and PC2 together account for a large share of the total variance in the dataset.\n",
    "- After PC5, the eigenvalues drop below 1, indicating that each subsequent component explains less variance than any single original feature.\n",
    "- The last few PCs (PC11–PC13) contribute very little additional information, as seen by their small bars.\n",
    "\n",
    "**What can people learn?**  \n",
    "- This plot helps determine how many principal components to retain for effective dimensionality reduction.\n",
    "- In this case, keeping the first 3–5 PCs would preserve most of the dataset's information, while reducing noise and complexity.\n",
    "- The sharp decline in eigenvalues after the first few PCs visually justifies focusing on those components for further analysis or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc15610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eigenvalues in a scree plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, len(eigenvalues) + 1), eigenvalues, color='teal')\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.title(\"Eigenvalues of Principal Components\")\n",
    "plt.xticks(range(1, len(eigenvalues) + 1))\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/images/Lab5_DanielTongu_EigenvaluesBarChart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acfb153",
   "metadata": {},
   "source": [
    "## 📈 Scree Plot – Cumulative Variance Explained\n",
    "\n",
    "**What does this image show?**  \n",
    "- The scree plot displays the cumulative proportion of variance explained as more principal components are added.\n",
    "- The x-axis shows the number of principal components (from 1 to 13), and the y-axis shows the cumulative variance explained (from 0 to 1).\n",
    "\n",
    "**What is its highlight?**  \n",
    "- The first principal component explains about **36%** of the total variance.\n",
    "- The first 2 components together explain about **55%** of the variance.\n",
    "- The first 3 components capture roughly **67%** of the variance, and the first 5 components together explain over **80%**.\n",
    "- By the time 7 components are included, more than **89%** of the variance is explained.\n",
    "- The curve flattens after the first few components, indicating diminishing returns for adding more components.\n",
    "\n",
    "**What can people learn?**  \n",
    "- Most of the dataset’s structure can be captured with just the first **3–5 principal components**, making dimensionality reduction highly effective.\n",
    "- You can reduce the dataset to 3 or 5 dimensions with minimal information loss, simplifying analysis and visualization while retaining the essential patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fae6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance Explained\")\n",
    "plt.title(\"Scree Plot - Cumulative Variance\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"/images/Lab5_DanielTongu_ScreePlot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22a87f",
   "metadata": {},
   "source": [
    "## 🌐 PCA 2D Scatterplot\n",
    "\n",
    "**What does this image show?**  \n",
    "- This scatterplot projects each heart disease sample onto the first two principal components (PC1 and PC2), with points colored by the target label (presence or absence of heart disease).\n",
    "- The x-axis (PC1) and y-axis (PC2) together capture about **33% of the total variance** in the dataset.\n",
    "- Each cluster corresponds to a different target class, revealing how PCA separates the classes in reduced dimensions.\n",
    "\n",
    "**What is its highlight?**  \n",
    "- **Target 0** (no heart disease) and **target 1** (heart disease) show visible structure and partial separation.\n",
    "- The plot demonstrates that even with just two principal components, much of the class structure is preserved:  \n",
    "    - PC1 is especially effective at separating the two groups.\n",
    "    - PC2 adds further separation and reveals additional structure.\n",
    "\n",
    "**What can people learn?**  \n",
    "- **PCA can reduce 13-dimensional data to 2D while retaining key differences between categories.**\n",
    "- The separation of classes shows that PCA is effective for visualizing and exploring class structure in high-dimensional datasets.\n",
    "- Some overlap suggests similarities between groups, but the overall structure is still informative for exploratory analysis and potential classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ba590",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=target, palette='Set2')\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA 2D Scatterplot\")\n",
    "plt.legend(title=\"Target\")\n",
    "plt.savefig(\"/images/Lab5_DanielTongu_PCA2DScatterplot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efde94f",
   "metadata": {},
   "source": [
    "## 🧭 PCA Biplot – Feature Vector Overlay\n",
    "\n",
    "**What does this image show?**  \n",
    "- The biplot overlays the PCA-transformed heart disease samples (as points) with red arrows representing the original features’ directions and contributions to the first two principal components (PC1 and PC2).\n",
    "- Each arrow’s direction and length indicate how strongly and in which direction a feature influences the principal components.\n",
    "\n",
    "**What is its highlight?**  \n",
    "- Features like **oldpeak** and **exang** have long arrows pointing along the PC1 axis, showing they contribute heavily to PC1 and help separate samples along this dimension.\n",
    "- **thalach** and **slope** have arrows with strong components in both PC1 and PC2, indicating they influence both axes.\n",
    "- The plot visually confirms that certain features drive separation between target classes, especially along PC1.\n",
    "\n",
    "**What can people learn?**  \n",
    "- Biplots allow you to interpret what each principal component represents in terms of the original features.\n",
    "- You can see which features are most important for distinguishing between heart disease classes:  \n",
    "    - For example, samples with high oldpeak and exang are pushed to the right in the plot (high PC1).\n",
    "    - Features with arrows pointing in similar directions are positively correlated.\n",
    "- This helps in understanding the underlying structure of the data and guides feature selection or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec722d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# Scatter plot of the PCA scores\n",
    "scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=target, cmap='viridis')\n",
    "\n",
    "# Plot arrows for feature loadings, scaled for visibility\n",
    "for i, feature in enumerate(Eigenvectors.index):\n",
    "    ax.arrow(0, 0, Eigenvectors.iloc[i, 0]*5, Eigenvectors.iloc[i, 1]*5, color='r', alpha=0.5, head_width=0.1)\n",
    "    ax.text(Eigenvectors.iloc[i, 0]*5.2, Eigenvectors.iloc[i, 1]*5.2, feature, color='r', ha='center', va='center')\n",
    "\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_title(\"Biplot of Principal Components\")\n",
    "plt.savefig(\"/images/Lab5_DanielTongu_Biplot.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9223a137",
   "metadata": {},
   "source": [
    "## ✅ Conclusion\n",
    "\n",
    "This notebook demonstrated the complete PCA workflow:\n",
    "- **Data standardization** to ensure fair comparison across features.\n",
    "- **Covariance matrix computation** to reveal relationships between features.\n",
    "- **Eigen decomposition** to obtain principal components and feature loadings (`Eigenvectors` DataFrame).\n",
    "- **Principal component selection** using eigenvalues and cumulative explained variance.\n",
    "- **Visualizations** including heatmap, eigenvalue bar chart, scree plot, 2D scatterplot, and biplot for interpretation.\n",
    "\n",
    "**Key takeaways:**  \n",
    "- PCA uncovers structure in high-dimensional data, enabling interpretable 2D/3D projections with minimal information loss.\n",
    "- This dimensionality reduction is valuable for exploratory analysis and machine learning, simplifying complex datasets while preserving essential patterns.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
